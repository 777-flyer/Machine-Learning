# -*- coding: utf-8 -*-
"""AdaBoost Classification

Automatically generated by Colab.

"""

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score


#avoid warning messages
import warnings
warnings.filterwarnings('ignore')



"""# Exploratory Data Analysis (EDA)

***Australian Rainy Weather Prediction
 Dataset:***

*   **Date**

*   **Location:**	49 different locations has been considered.

*   **MinTemp:** Minimum temp in degree celcius

*   **MaxTemp:** Maximum temp in degree celcius

*   **Rainfall:**	The amount of rainfall recorded for the day in mm.

*   **Evaporation:**	Evaporation(mm) in 24hours


*   **Sunshine:**	number of hours of bright sunshine.

*   **WindGustDir:**	The direction of the strongest wind gust in the 24 hours to midnight

*   **WindGustSpeed:**	The speed (km/h) of the strongest wind gust in the 24 hours to midnight

*   **WindDir9am, WindDir3pm:**	Direction of wind at 9am and 3pm

*   **WindSpeed9am, WindSpeed3pm:** Speed of wind at 9am and 3pm

*   **Humidity9am, Humidity3pm:** Humidity at 9am and 3pm
*   **Pressure9am, Pressure3pm:** Atmospheric pressure at 9am and 3pm

*   **Cloud9am, Cloud3pm:** Fraction of sky obscured by cloud at 9am

*   **Temp9am, Temp3pm:** Temperature at 9am and 3pm

*   **RainToday:**
         1 = rained today
         0 = didn't rain today
*   **RainTomorrow [Target column]:**          
         
         1 = Will be raining tomorrow
         0 = won't be raining tomorrow
"""

# read dataset from a public Google Drive File

file_link = ''

# get the id part of the file
id = file_link.split("/")[-2]

# creating a new link using the id for better readability of csv file in pandas
new_link = f'https://drive.google.com/uc?id={id}'
df = pd.read_csv(new_link)

# let's look at the first few instances
df.head(5)

df.shape

df.info()

df.describe(include='all')

df.nunique()

df.isnull().sum()

cat_cols=df.select_dtypes(include=['object']).columns.tolist()
num_cols = df.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

# Visualize the distribution of the 'RainTomorrow' variable

print(df.value_counts('RainTomorrow'))

plt.figure(figsize=(5, 3))
df['RainTomorrow'].value_counts().plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Rain Prediction')
plt.xlabel('Rained')
plt.ylabel('Days count')
plt.xticks(rotation = 0)
plt.show()

# Correlation matrix plot
plt.figure(figsize=(8, 6))
correlation_matrix = df.corr(numeric_only=True)
# print(correlation_matrix)
plt.matshow(correlation_matrix)
plt.title('Correlation Matrix')
plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)
plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
plt.colorbar()
plt.show()

"""# Data Prepocessing"""

#drop redundant feature(s)
df = df.drop(['Date'], axis=1)
df

#drop duplicates
df = df.drop_duplicates()
df

# Drop rows with missing values
df.dropna(subset=['WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'], inplace=True)
df

# Fill missing values in numerical features with column's mean value
columns_to_fill = [
    'WindSpeed9am', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation',
    'Sunshine', 'WindGustSpeed', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm',
    'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm'
]
df[columns_to_fill] = df[columns_to_fill].fillna(df[columns_to_fill].mean())
df

df.shape

#check for total null value for each feature
df.isnull().sum()

#check for categorical and numerical variables in df
cat_cols=df.select_dtypes(include=['object']).columns.tolist()
num_cols = df.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

#converting categorical variables to numerical
df = pd.get_dummies(df, columns=['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday'])
df.isnull().sum()
df.shape

df.head()

df.shape

"""# Implementing AdaBoost using Scikit-learn Library

sklearn AdaBoost reference: https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
"""

# Split the data into 80-20 train-test split (you can do other ratios as well)
X = df.drop(['RainTomorrow'], axis=1)
# df['RainTomorrow']= df['RainTomorrow'].replace(0, -1)
y = df['RainTomorrow']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# display the shapes

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

abc = AdaBoostClassifier(n_estimators=5,learning_rate=1, random_state = 42)
model = abc.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Accuracy score (training): {0:.3f}".format(model.score(X_train, y_train)))
print("Accuracy score (validation): {0:.3f}".format(model.score(X_test, y_test)))

abc = AdaBoostClassifier(n_estimators=100,learning_rate=0.5, random_state = 42)
model = abc.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("Accuracy score (training): {0:.3f}".format(model.score(X_train, y_train)))
print("Accuracy score (validation): {0:.3f}".format(model.score(X_test, y_test)))

"""
# Implementing Adaboost from Scratch
##Steps:
### 1. Assign Weights to the Dataset
- Start by giving **equal weight** to every sample:  
  $w_i = \frac{1}{N} \quad \text{for } i = 1, 2, ..., N$

### 2. Create Stumps (Weak Learners)
- Use **Decision Stumps**, i.e., Decision Trees with `max_depth = 1`.
- Use **Gini Index** to find the best split at each step.

### 3. For Each Iteration (Round of Boosting)

#### a. **Train the Weak Learner**
- Fit a stump on the training data using the current sample weights.

#### b. **Predict and Compute Error**
- Calculate the weighted classification error:
  $$
  \text{error}_t = \sum_{i=1}^{N} w_i \cdot \mathbb(h_t(x_i) \ne y_i)
  $$
  Here,
  
  $w_i$ is the weight of instance $i$
  
  $h_t(x_i)$ is the prediction of weak classifier at iteration $t$

  $y_i$ is the true label

  $(h_t(x_i) \ne y_i)$ returns 1 if the condition is true and 0 otherwise

#### c. **Compute the "Amount of Say" (Alpha)**
- This is the weight given to the weak learner:
  $$
  \alpha_t = \eta \cdot \frac{1}{2} \log\left(\frac{1 - \text{error}_t}{\text{error}_t}\right)
  $$
- Where:
  - $\eta$ is the **learning rate** (e.g., 0.1, 0.5, 1.0)

### 4. Modify the Sample Weights

- **Increase** the weights of **incorrectly classified** samples:
  $$
  w_i \leftarrow w_i \cdot e^{\alpha_t}
  $$

- **Decrease** the weights of **correctly classified** samples:
  $$
  w_i \leftarrow w_i \cdot e^{-\alpha_t}
  $$

- Combined in one expression:
  $$
  w_i \leftarrow w_i \cdot e^{\alpha_t \cdot (2 \cdot(y_i \ne h_t(x_i)) - 1)}
  $$

### 5. Normalize the Weights

Ensure the weights sum to 1:
$$
w_i \leftarrow \frac{w_i}{\sum_{j=1}^N w_j}
$$

### 6. Final Prediction (Ensemble Output)

The final prediction is a **weighted vote** of all weak learners:
$$
F(x) = \sum_{t=1}^T \alpha_t \cdot h_t(x)
$$

Classify based on the sign:
$$
\hat{y} = \text{sign}(F(x))
$$

### Notes:
- Ensure labels are in $\{-1, +1\}$ for proper calculations."""

from sklearn.tree import DecisionTreeClassifier # To create the stumps

class AdaBoostClassifier:
    def __init__(self, n_estimators=50, learning_rate=1.0):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.alphas = []
        self.models = []

    def fit(self, X, y):
        # equal weights to all samples
        n_samples = X.shape[0]
        weights = np.ones(n_samples) / n_samples

        for t in range(self.n_estimators):
            # creating stump (weak learner)
            stump = DecisionTreeClassifier(max_depth=1, random_state=42)

            # Training the weak learner with current weights
            stump.fit(X, y, sample_weight=weights)

            # prediction and error
            predictions = stump.predict(X)

            # weighted error calc
            incorrect = (predictions != y).astype(int)
            error = np.sum(weights * incorrect)

            # !! division by zero
            if error >= 1 or error <= 0:
                break

            # Amount of Say - [alpha]
            alpha = self.learning_rate * 0.5 * np.log((1 - error) / error)

            # Storing the model and alpha
            self.models.append(stump)
            self.alphas.append(alpha)

            # modifying the sample weights
            for i in range(n_samples):
                if predictions[i] != y[i]:
                    # inc: e^(alpha)
                    weights[i] = weights[i] * np.exp(alpha)
                else:
                    # cor: e^(-alpha)
                    weights[i] = weights[i] * np.exp(-alpha) #

            # normalizing weights
            weights = weights / np.sum(weights)
            #pass

    def predict(self, X):
        # final prediction as zeros
        final_pred = np.zeros(X.shape[0])

        # weighted vote of all weak learners
        for alpha, model in zip(self.alphas, self.models):
            predictions = model.predict(X)
            final_pred += alpha * predictions

        # Classification based on sign
        return np.sign(final_pred).astype(int)
        #pass

# Convert labels to {-1, 1} instead of {0,1}
y_train_ab = y_train.replace({0: -1, 1: 1})
y_test_ab = y_test.replace({0: -1, 1: 1})

# Train AdaBoost from scratch
adaboost = AdaBoostClassifier(n_estimators=50, learning_rate = 1)
adaboost.fit(X_train, y_train_ab.to_numpy())

# Predict and evaluate
y_pred_ab = adaboost.predict(X_test)

print("AdaBoost Classifier (from scratch) Accuracy:", accuracy_score(y_test_ab, y_pred_ab))

#Assign weights to the dataset
        #weights = ?