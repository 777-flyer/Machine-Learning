# -*- coding: utf-8 -*-
"""DecisionTree & RandomForest.ipynb

Automatically generated by Colab.

"""

#importing the necessary libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# ignore the warning messages
import warnings
warnings.filterwarnings('ignore')

"""# Exploratory Data Analysis (EDA)

***Titanic Dataset:***

*   **PassengerId**

*   **Pclass:**	Ticket class. A proxy for socio-economic status (SES).
                 1 = 1st (Upper)
                 2 = 2nd (Middle)
                 3 = 3rd (Lower)

*   **Name**

*   **Sex**

*   **Age:**	Age in years. Age is fractional if less than 1. If the age is estimated, is it in the form of **xx.5**	.

*   **SibSp:**	number of siblings / spouses aboard the Titanic. The dataset defines family relations in this way:

                 Sibling = brother, sister, stepbrother, stepsister
                 Spouse = husband, wife (mistresses and fiancÃ©s were ignored)


*   **Parch:**	number of parents / children aboard the Titanic. The dataset defines family relations in this way:
                 Parent = mother, father
                 Child = daughter, son, stepdaughter, stepson
                 [Some children travelled only with a nanny, therefore parch=0 for them.]

*   **Ticket:**	Ticket number

*   **Fare:**	Passenger fare

*   **Cabin:**	Cabin number

*   **Embarked:**	Port of Embarkation
                 C = Cherbourg
                 Q = Queenstown
                 S = Southampton

*   **Survived:**	Survival Status
                 0 = No
                 1 = Yes
"""

# read dataset from a Google Drive File

file_link = '' 

# get the id part of the file
id = file_link.split("/")[-2]
# print(id)

# creating a new link using the id so that we can easily read the csv file in pandas
new_link = f'https://drive.google.com/uc?id={id}'
print(new_link)
df = pd.read_csv(new_link)

# let's look at the first few instances
df.head(10)

df.shape

df.info()

df.describe(include='all')

# Check Unique Values
df.nunique()

# Check for missing values
df.isnull().sum()

#check the cat and num column name
cat_cols = df.select_dtypes(include=['object']).columns.tolist()
num_cols = df.select_dtypes(include=np.number).columns.tolist()
print("Categorical Variables:")
print(cat_cols)
print("Numerical Variables:")
print(num_cols)

# Visualize the distribution of the 'Survived' variable

print(df.value_counts('Survived'))

plt.figure(figsize=(5, 3))
df['Survived'].value_counts().plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Survival Distribution')
plt.xlabel('Survived')
plt.ylabel('Survival Count')
plt.xticks(rotation = 45)
plt.show()

# Visualize the distribution of 'Pclass' variable
print(df.value_counts('Pclass').sort_index())

plt.figure(figsize=(5, 3))
df['Pclass'].value_counts().sort_index().plot(kind='bar', color='slategrey')
plt.title('Passenger Class Distribution')
plt.xlabel('Pclass')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# Visualize the distribution of 'Sex' variable
print(df.value_counts('Sex'))

plt.figure(figsize=(5, 3))
df['Sex'].value_counts().plot(kind='bar', color=['dimgray', 'magenta'])
plt.title('Gender Distribution')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

# Visualize the distribution of 'Age' variable
plt.figure(figsize=(8, 5))
plt.hist(df['Age'].dropna(), edgecolor='black')
# dropna eliminates the null/missing values but not in df
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

# Visualize the relationship between 'Pclass' and 'Survived'

plt.figure(figsize=(6, 4))
pd.crosstab(df['Pclass'], df['Survived']).plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Survival by Passenger Class')
plt.xlabel('Pclass')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# Visualize the relationship between 'Sex' and 'Survived'
plt.figure(figsize=(6, 4))
pd.crosstab(df['Sex'], df['Survived']).plot(kind='bar', color=['lightcoral', 'lightgreen'])
plt.title('Survival by Gender')
plt.xlabel('Sex')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

# Correlation matrix plot
plt.figure(figsize=(8, 6))
correlation_matrix = df.corr(numeric_only=True)
print(correlation_matrix)
plt.matshow(correlation_matrix)
plt.title('Correlation Matrix')
plt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns, rotation=90)
plt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns)
plt.colorbar()
plt.show()

"""# Data Preprocessing"""

# Drop redundant features/columns
df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1) #axis=1 is column, 0 is row
df

# Drop duplicates
df = df.drop_duplicates()
df

# Handle missing values in the column "Age" by replacing with the mean value
df['Age'].fillna(df['Age'].mean(), inplace = True) #changing df permanently
df

# Drop rows with missing values
df.dropna(subset=['Embarked'], inplace=True)
df

df.shape

df.isnull().sum()

# Convert categorical variables to numerical (one-hot encoding)
df = pd.get_dummies(df, columns=['Sex', 'Embarked'])
df

df.shape

"""# Implementing Decision Tree and Random Forest Using Scikit-learn Library"""

# Split the data into 80-20 train-test split

X = df.drop(['Survived'], axis=1) # features
y = df['Survived'] # target column

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# display the shapes

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Implementing Decision Tree Classifier
decision_tree_model = DecisionTreeClassifier(criterion='gini', max_depth = 10, random_state=42)
decision_tree_model.fit(X_train, y_train)

# Predictions on the test set
y_pred_dt = decision_tree_model.predict(X_test)

# Evaluate the model
print("Decision Tree Classifier Accuracy:", accuracy_score(y_test, y_pred_dt))
#accuracy = no.of correctly predicted data / total test data

# Visualize the Decision Tree
plt.figure(figsize=(60, 40))
plot_tree(decision_tree_model, feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True, rounded=True)
plt.title('Decision Tree Visualization')
plt.show()

# Implementing Random Forest Classifier
random_forest_model = RandomForestClassifier(n_estimators=100, max_depth = 5, criterion='gini', bootstrap=True, random_state=42)
random_forest_model.fit(X_train, y_train)

# Predictions on the test set
y_pred_rf = random_forest_model.predict(X_test)

# Evaluate the model
print("Random Forest Classifier Accuracy:", accuracy_score(y_test, y_pred_rf))

# Visualize one of the decision trees (first tree) in the Forest
plt.figure(figsize=(60, 40))
plot_tree(random_forest_model.estimators_[0], feature_names=X.columns, class_names=['Not Survived', 'Survived'], filled=True, rounded=True)
plt.title('First Decision Tree Visualization (from Random Forest)')
plt.show()

"""# Implementing Decision Tree and Random Forest from Scratch"""

class DecisionTree:

    def __init__(self, max_depth = None):

        self.max_depth = max_depth

    class Node:

        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):

            self.feature = feature
            self.threshold = threshold
            self.left = left
            self.right = right
            self.value = value

    def fit(self, X, y, depth = 0):

        if len(set(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):

            unique, counts = np.unique(y, return_counts=True)
            majorityClass = unique[np.argmax(counts)]
            return self.Node(value=majorityClass)

        best_feature, best_threshold = self.find_best_split(X, y)

        if best_feature is None:

            unique, counts = np.unique(y, return_counts=True)
            majorityClass = unique[np.argmax(counts)]
            return self.Node(value=majorityClass)

        # Split the data into left and right subsets based on the best split
        left_mask = X[:, best_feature] <= best_threshold
        right_mask = X[:, best_feature] > best_threshold

        x_left, y_left = X[left_mask], y[left_mask]
        x_right, y_right = X[right_mask], y[right_mask]

        if len(y_left) == 0 or len(y_right) == 0:
            unique, counts = np.unique(y, return_counts=True)
            majorityClass = unique[np.argmax(counts)]
            return self.Node(value=majorityClass)

        # Recursively build the tree
        left_node = self.fit(x_left, y_left, depth + 1)
        right_node = self.fit(x_right, y_right, depth + 1)

        # return a decision node with feature, threshold, left_node, and right_node
        return self.Node(feature=best_feature, threshold=best_threshold, left=left_node, right=right_node)

    def find_best_split(self, X, y):
        samples, features = X.shape
        best_gini = float('inf')
        best_feature = None
        best_threshold = None

        for feature in range(features):
            values = np.unique(X[:, feature])

            for threshold in values:

                left_mask = X[:, feature] <= threshold
                right_mask = X[:, feature] > threshold

                left_count = np.sum(left_mask)
                right_count = np.sum(right_mask)

                if left_count == 0 or right_count == 0:
                    continue

                gini_left = self.calculateGini(y[left_mask])
                gini_right = self.calculateGini(y[right_mask])

                total = left_count + right_count
                gini = (left_count / total) * gini_left + (right_count / total) * gini_right

                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def calculateGini(self, y):

        if len(y) == 0:
            return 0
        unique, counts = np.unique(y, return_counts=True)
        probabilities = counts / len(y)
        gini = 1 - np.sum(probabilities ** 2)
        return gini

    def predict(self, node, x):

        if node.value is not None:
            return node.value
        if x[node.feature] <= node.threshold:
            return self.predict(node.left, x)
        else:
            return self.predict(node.right, x)



# converting Pandas dataframe to NumPy Array
X_train_np = X_train.to_numpy()
y_train_np = y_train.to_numpy()
X_test_np = X_test.to_numpy()
y_test_np = y_test.to_numpy()


# Create the model and fit the training data
decision_tree = DecisionTree(max_depth=5)
final_tree = decision_tree.fit(X_train_np, y_train_np) # returns the information of the final tree in a suitable data structure

# Predictions on the test set
y_pred_dt_scr = np.array([decision_tree.predict(final_tree, instance) for instance in X_test_np])

# Calculate and print the accuracy using y_test and y_pred manually
accuracy_dt_scr = np.sum(y_pred_dt_scr == y_test_np) / len(y_test_np)

print("Decision Tree Classifier (from scratch) Accuracy:", accuracy_dt_scr)

class RandomForest:
    def __init__(self, n_estimators = 100, max_depth = None):
        # Initialize an empty list of trees
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.trees = []

    def fit(self, X, y):

        # Loop upto n_estimators:
        #     Create a bootstrap sample (X_sample, y_sample) from the training data
        #     DT_model = DecisionTree(max_depth) [reuse the DecisionTree class above]
        #     tree = DT_model.fit(X_sample, y_sample)
        #     Append tree to the list of trees

        for _ in range(self.n_estimators):

            indices = np.random.choice(len(X), len(X), replace=True)
            X_sample = X[indices]
            y_sample = y[indices]
            DT_model = DecisionTree(max_depth=self.max_depth)
            tree = DT_model.fit(X_sample, y_sample)
            self.trees.append((DT_model, tree))

    def predict(self, X):
        # Use majority voting to predict the class for the given instance using all trees
        predictions = []
        for x in X:
            tree_preds = [model.predict(tree, x) for model, tree in self.trees]
            unique, counts = np.unique(tree_preds, return_counts=True)
            predictions.append(unique[np.argmax(counts)])
        return np.array(predictions)


# Create the model and fit the training data
random_forest = RandomForest(n_estimators=25, max_depth=5)
random_forest.fit(X_train_np, y_train_np)

# Predictions on the test set
y_pred_rf_scr = random_forest.predict(X_test_np)

# Calculate the accuracy like before
accuracy_rf_scr = np.sum(y_pred_rf_scr == y_test_np) / len(y_test_np)

print("Random Forest Classifier (from scratch) Accuracy:", accuracy_rf_scr)